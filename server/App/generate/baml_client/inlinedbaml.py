###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off

file_map = {

    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> GPT4Mini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    mutliplier 1.5\n    max_delay_ms 10000\n  }\n}\n",
    "generate.baml": "\nclass GenParams {\n  name string\n  description string\n  test_case map<string, string>\n  variables string[]\n}\n\n\nenum GenRole {\n  System\n  User\n  Assistant\n}\n\n\nclass GenTestCase {\n  variables map<string, string>\n}\n\nclass GenPromptMessage {\n  role GenRole\n  content string\n}\n\nfunction GenerateTestCases(params: GenParams) -> GenTestCase[] {\n    client GPT4Mini\n    prompt #\"\n        To test out a function I wrote I require additional testing data.\n        The function is called {{params.name}}. Here is a short description of the function:\n        {{params.description}}\n\n        Generate up to two data points for my function that each contain a value for the following fields:\n        {% for var in params.variables %}\n        {{ var }},\n        {% endfor %}\n\n        Here is an example:\n        {{params.test_case}}\n\n\n        If the provided information is not sufficient to generate data points, simply output \"insufficient context\".\n        Do not exactly follow the example provided. The data points you generate may differ from the example but they must contain the same fields.\n    \"#\n}\n\n\nfunction GeneratePromptMessages(params: GenParams) -> GenPromptMessage[] {\n  client GPT4Mini\n\n  prompt #\"\n    I defined an AI function that takes a set of parameters as input and returns a specific output. Now I must define a prompt for this function.\n    A prompt consists of several prompt messages. Prompt messages follow the OpenAI specification. Each prompt message has a role: \"user\", \"system\" or \"assistant\" and content as a string.\n    \n    Each parameter of the AI function must be contained in at least one prompt message in between two sets of braces.\n\n    Here is an example of a prompt for the AI Function “Translate Text” that translates text to a target language, which takes the parameters: “text” and “language”:\n    [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a translation AI that accurately translates text into specified target languages.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Translate the following text into {% raw %}{{language}}{% endraw %}: {% raw %}{{text}}{% endraw %}\"\n        }\n    ]\n\n    Now generate a prompt for the AI function: {{params.name}}.\n    Here is a short description of the function: {{params.description}}.\n\n    The function takes the following parameters as input:\n    {% for var in params.variables %}\n    {{ var }},\n    {% endfor %}\n\n    Here are example values for each parameter:\n    {{params.test_case}}\n\n    If the provided context is not sufficient to generate a prompt simply output: \"insufficient context\".\n  \"#\n\n}\n",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.67.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
}

def get_baml_files():
    return file_map
